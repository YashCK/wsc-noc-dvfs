\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Priority-Aware NoC DVFS under Power Caps\\
}

\author{\IEEEauthorblockN{Yash Kodali}
\IEEEauthorblockA{\textit{University of California, Berkeley} \\
yash.kodali@berkeley.edu}
\and
\IEEEauthorblockN{Evan Li}
\IEEEauthorblockA{\textit{University of California, Berkeley} \\
evan.li@berkeley.edu}
\and
\IEEEauthorblockN{Nathan Sutherland}
\IEEEauthorblockA{\textit{University of California, Berkeley} \\
nathan.sutherland@berkeley.edu}
}

\maketitle

\begin{abstract}
As warehouse-scale systems increasingly rely on power capping to manage energy costs and infrastructure limits, Network-on-Chip (NoC) resources must be throttled efficiently. However, conventional uniform Dynamic Voltage Frequency Scaling (DVFS) policies fail to distinguish between latency-sensitive control traffic and throughput-oriented batch workloads, leading to severe tail-latency violations. As such, there is a need for strategies that enable power-constrained NoCs to provide performance efficiency for traffic classes without violating global power caps.

This paper presents a Priority-Aware NoC DVFS framework designed to enforce power caps while selectively preserving the performance of critical traffic. We utilize the SNIPER multi-core simulator to generate network traces from both batch and control class benchmarks from PARSEC and TailBench++ respectively. These traces are evaluated through a cycle-accurate BookSim 2.0 interconnection network simulation to compare three distinct control paradigms: a reactive hardware controller, a queue-theoretic PID controller, and a model-predictive performance-targeted controller.

Our experimental results show that TODO. This work provides a scalable path for managing NoC power without compromising the responsiveness of critical data center services.
\end{abstract}

\begin{IEEEkeywords}Warehouse-scale computing, power capping, network-on-chip, DVFS, tail latency, quality of service\end{IEEEkeywords}

\section{Introduction}
Warehouse-scale computers (WSCs) increasingly operate under \emph{site-level} power caps imposed by electrical provisioning and cooling limits. When aggregate demand approaches these limits, operators enforce caps by throttling lower-priority work so that user-facing services continue to meet strict tail-latency service-level objectives (SLOs) \cite{li_capmaestro_hpca19,li_thunderbolt_osdi20,bhattacharya_speed_stability_igcc12}. At the processor level, the same constraints appear as tight power/thermal envelopes that motivate fine-grained power management inside the chip. As core counts increase, on-chip data movement and interconnect activity become a major contributor to system power and performance \cite{adhinarayanan_interconnect_power_iiswc16}.

One straightforward way to enforce a chip power cap is to throttle components uniformly. In the context of NoCs, \emph{uniform throttling} means reducing the frequency (and voltage) of all routers by the same factor to meet a power limit. While effective at capping power, this approach can needlessly degrade performance – especially for latency-critical traffic - because it slows down every network packet uniformly. In many applications, on-chip traffic is heterogeneous: certain control messages require low latency, whereas batch or bulk data transfers can better tolerate delays. Treating all network traffic the same under a power cap fails to account for these differing requirements. As a result, latency-sensitive transactions can suffer unnecssary slowdowns, potentially violating  service-level objectives (SLOs), even if other parts of the network have slack.

We can intelligently throttle less critical traffic while preserving performance for critical flows, via Dynamic Voltage and Frequency Scaling (DVFS) on the NoC. DVFS allows modulating the supply voltage and clock frequency of router hardware, trading off performance for power savings in a controllable way ~\cite{hesse_coherence_dvfs}. By lowering frequency/voltage on parts of the NoC, dynamic power consumption can be reduced (roughly quadratically with voltage). Our approach is to leverage fine-grained DVFS domains in the NoC to balance network latency and saving enough power to meet the cap. We adjust the frequency of different region domains or individual routers, independently, based on their workload and traffic class. For example, routers primarily handling best-effort batch traffic can be throttled more when needed, freeing power headroom to keep the critical control traffic flowing with low latency.

We explore the question of how a priority-aware NoC DVFS policy can keep control-class packets within their SLO while meeting the power budget, using a trace-driven, cycle-accurate methodology: a 64-core Sniper configuration generates class-tagged NoC packet traces, and BookSim 2.0 simulates a mesh/flatfly NoC with multi-domain DVFS actuation \cite{carlson_sniper_sc11,jiang_booksim_ispass13}. We compare a uniform-throttling baseline against three families of controllers: (i) a low-cost reactive hardware-style policy, (ii) a queue-theoretic feedback controller, and (iii) an optimization performance-targeted controller. We study how these control policies affect the latency and stability of network traffic across different topologies, packet injection rates, and power caps.

\section{Problem Setting}

\subsection{Control vs.\ batch traffic and SLO definition}
We model two packet classes. \emph{Control-class} packets (class 0 in our traces) represent latency-sensitive messages whose delay can directly stall cores or elongate critical paths (e.g., coherence commands/acks, synchronization). \emph{Batch-class} packets (class 1) represent throughput-oriented transfers (e.g., bulk data movement or cache-line payload responses) that can tolerate additional latency. This two-class abstraction matches WSC power-management practice, where batch work is typically slowed first during power emergencies \cite{li_thunderbolt_osdi20}. Since absolute cycle targets vary by microarchitecture, we define the control SLO \emph{relative to a practical baseline}: do no worse than uniform NoC throttling at the same power cap. A priority-aware policy is successful if it stays power-safe while reducing control-class \texttt{P99} versus uniform DVFS, accepting some batch performance loss if needed.

\subsection{Microarchitectural Considerations: Flit Reordering and FIFOs}
\label{sec:flit-reorder-fifos}

\paragraph{Why this matters under NoC DVFS}
Applying DVFS to routers and links changes the \emph{service rate} of buffers and arbiters without changing the \emph{arrival process} (packet injection). When DVFS is applied heterogeneously across domains (e.g., per-router or per-region), adjacent routers may operate at different effective speeds. This creates transient \emph{rate mismatch} at domain boundaries, where an upstream router can deliver flits faster than the downstream router can drain them (or vice versa). The mismatch amplifies queueing and can manifest as head-of-line (HoL) blocking for latency-critical control traffic, even if average utilization is moderate. Therefore, DVFS-aware design must explicitly consider buffering, backpressure, and ordering constraints.

\paragraph{Flit-level Behavior and Ordering}
In wormhole routers, packets are decomposed into flits that traverse the network subject to flow control. Many architectures preserve in-order delivery per flow/VC, but DVFS-induced stalls can cause flits of different packets and/or different classes to interleave differently at routers, especially when:
(i) adaptive routing is enabled,
(ii) VCs are shared across classes, or
(iii) arbitration priorities change dynamically based on class-aware policies.
If the model allows multiple paths or multiple outstanding packets per flow, differences in per-hop service rate can also create packet-level reordering at the destination unless additional constraints are enforced.

\paragraph{FIFO design points in the simulator/model.}
To keep the evaluation faithful while avoiding artifacts, we adopt the following model assumptions:
\begin{itemize}
  \item \textbf{Per-VC input FIFOs.} Each input port maintains per-VC FIFOs with credit-based backpressure; DVFS scales the dequeue/service rate (pipeline progress) but does not alter credit semantics.
  \item \textbf{Class separation.} Control (class~0) and batch (class~1) traffic are either assigned dedicated VCs or use a shared VC with strict-priority arbitration. Dedicated VCs eliminate cross-class HoL blocking at the expense of VC resources; shared VCs more closely mimic deployments where QoS is enforced by priority alone.
  \item \textbf{Boundary buffering.} At DVFS domain boundaries, we ensure sufficient buffering to absorb short rate mismatches. In practice, this is modeled via input FIFO depth and/or an explicit boundary FIFO. 
\end{itemize}

\paragraph{Implication for control objectives}
These microarchitectural effects directly tie into our control policy. A DVFS reduction in a congested region increases effective queuing delay superlinearly near saturation, so policies that rely purely on average occupancy can inadvertently violate control-class P99. Conversely, targeted DVFS increases in hot spots can ``unlock'' blocked FIFOs and reduce tail latency without increasing system-wide power, motivating domain-level actuation rather than uniform throttling.

\section{Related Work}
\subsection{Priority-aware power capping in WSCs}
WSC power capping must account for workload priority to protect user-facing tail latency. CapMaestro proposes a scalable, closed-loop, priority-aware control architecture that shifts power budget from low-priority to high-priority work across the power distribution hierarchy \cite{li_capmaestro_hpca19}. Thunderbolt targets cluster-scale power caps by throttling batch tasks ``just enough'' while preserving serving QoS, enabling safe power oversubscription \cite{li_thunderbolt_osdi20}. At the single-node level, Rubik demonstrates that fast analytical models can guide power allocation to protect latency-critical work under a cap \cite{kasture_rubik_micro15}. Bhattacharya \emph{et al.} highlight that data center power demand can change faster than existing controllers can react, and that instability under power capping can cause dramatic latency degradation \cite{bhattacharya_speed_stability_igcc12}. Rubik demonstrates that fast, model-based power control can preserve tail latency for colocated latency-critical services while improving efficiency \cite{kasture_rubik_micro15}.

\subsection{NoC DVFS and power budgeting}
NoC DVFS has been studied extensively, often using utilization or buffer occupancy driven reactive policies.Early closed-loop formulations for NoC power management treat DVFS as a control problem with estimation and feedback \cite{simunic_power_noc_tvlsi04}. Hesse \emph{et al.} argue that purely reactive DVFS is limited and improve prediction by leveraging coherence protocol information to anticipate NoC demand \cite{hesse_coherence_dvfs}. At the budgeting level, PEPON proposes hierarchical allocation of a chip-wide power budget to NoC routers to improve performance under a cap \cite{sharifi_pepon_pact12}. Optimization and control-inspired approaches for multi-VFI systems also exist \cite{bogdan_optimal_control_todaes12}. Our work differs in focus: we study priority-aware NoC DVFS specifically through the lens of WSC-style control-vs-batch QoS under a hard power cap, with tail-latency as the primary metric.

\subsection{NoC QoS and traffic differentiation}
QoS mechanisms such as priority arbitration, virtual-channel partitioning, and bandwidth reservation are also widely used to isolate latency-sensitive traffic. Commercial on-chip fabrics may expose multiple traffic classes to reduce queuing for critical messages \cite{amd_versal_qos}. Our approach is complementary: QoS scheduling reduces contention \emph{given a fixed service rate}, whereas DVFS changes the service rate itself. Under a power cap, priority-aware DVFS can be viewed as redistributing limited service capacity to better satisfy control-traffic tail-latency constraints.

\section{Understanding Power Caps in WSCs}
Datacenter operators provision power delivery and cooling for a fixed envelope, then enforce \emph{site-level} caps to avoid overload, reduce operating cost, and enable safe oversubscription. Real power demand can change rapidly, and naive throttling can destabilize latency-sensitive services \cite{bhattacharya_speed_stability_igcc12}. As a result, modern WSC power managers explicitly differentiate priorities, throttling batch work more aggressively than serving traffic during cap events \cite{li_capmaestro_hpca19,li_thunderbolt_osdi20}.

At the node level, site-level constraints translate into per-server or per-socket budgets. Hardware mechanisms and software control loops allocate these budgets across subsystems (cores, caches, memory, and fabrics). In this paper we focus on the NoC share of the node budget and treat it as a hard cap over the policy decision epoch. All policies are evaluated under the \emph{same} NoC power cap to isolate the impact of allocation decisions on control-class tail latency.

\section{NoC DVFS Background}
Dynamic voltage and frequency scaling (DVFS) trades performance for power by adjusting supply voltage $V$ and clock frequency $f$. Dynamic power scales approximately as $P \propto V^{2}f$, so reducing both can substantially cut power at the cost of lower service rate. Applied to a NoC, DVFS controls each router's \emph{service capacity}: lowering $f$ increases per-hop serialization latency and can increase queueing delay when offered load approaches service capacity.

\subsection{Actuation granularity}
DVFS can be applied globally (one domain for the entire NoC) or in multiple voltage-frequency islands that scale independently. Finer granularity enables targeted throttling: domains that primarily carry batch traffic can be slowed while preserving frequency along control-critical routes. However, finer granularity also increases hardware cost and can complicate control. Due to a limitation of time, we focus our evaluation of router-level domain control to understand and quantify the returns of our control policies.

\subsection{Transition latency and control stability}
DVFS transitions are not instantaneous: voltage changes require regulator settling, and frequent toggling can waste time and harm latency. At WSC scale, demand variability and control-loop stability are central challenges for power capping \cite{bhattacharya_speed_stability_igcc12}; similar issues appear on-chip when DVFS is driven by bursty injection and localized congestion. 

\paragraph{Hysteresis, quantization, and actuation granularity.}
NoC DVFS typically supports only a finite set of frequency points and may apply per-domain rather than per-router in real systems. Quantization can cause limit cycles when the operating point hovers near a threshold. To mitigate this, we incorporate:
\begin{itemize}
  \item \textbf{Hysteresis} on occupancy/latency thresholds to prevent frequent toggling,
  \item \textbf{Rate limiting} (e.g., max $\Delta f$ per epoch) to reduce oscillations,
  \item \textbf{Minimum dwell time} at a DVFS point to amortize transition overhead.
\end{itemize}

\paragraph{Mapping DVFS to NoC service.}
In the model, DVFS scales the effective router pipeline rate and link traversal rate, thereby scaling the \emph{service rate} $\mu$ of queues. From a queueing perspective, reducing frequency increases the utilization $\rho=\lambda/\mu$, and tail latency grows rapidly as $\rho \to 1$. This motivates policies that:
(i) protect control traffic via an SLO-aware override (boost when control P99 approaches the SLO), and
(ii) avoid globally reducing $\mu$ uniformly when congestion is localized.

\subsection{DVFS as a NoC service control knob}
From a queuing perspective, each DVFS decision changes the service rate $\mu$ of the router pipeline. When the offered load $\lambda$ is low, reducing $f$ primarily increases serialization latency. When $\lambda$ approaches $\mu$, queuing delay dominates and tail latency can rise sharply. Priority-aware DVFS aims to keep $\mu$ high where it most benefits control-class \texttt{P99} (hot spots and critical routes), while reducing $\mu$ in regions that primarily affect batch throughput.

\section{Infrastructure}
\subsection{Overview}
The simulation framework framework follows a multi-stage process that separates DVFS policy development with trace generation. This separation allows us to test to tweak and experiment with policies quickly without having to re-run the entire benchmark every time. First, we feed control and batch class benchmarks into an x86 simulator, SNIPER to get network traces. Then, we feed those network traces as well as a custom DVFS policy into a fast network simulator, BookSim 2.0 that captures metrics.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{infra_flow.png}
    \caption{Intrastructure Setup Flowgraph}
    \label{fig:your_label}
\end{figure}

\subsection{Benchmarks}
To evaluate how power management affects different types of data center traffic, we needed to simulate two specific types of workloads: Control-class and Batch-class. In modern data centers, these different tasks often run on the same hardware. However, they have very different goals. Control-class tasks, like web search or voice recognition, need to be extremely low latency. Batch-class tasks, like compressing files or processing images, need high throughput but the short-term time requirement to finish execution is not as important. By testing both together, we can see if aggressive power saving for the batch tasks affects latency-sensitive control jobs.

For our benchmarks, we chose TailBench++ for the control class and PARSEC for the batch class. From TailBench++, we used Xapian (search) and Sphinx (speech) because they act like real internet services that get bursts of user requests. From PARSEC, we used VIPS and Dedup because they move a lot of data around, which puts a heavy load on the Network-on-Chip (NoC). This setup lets us test if our power-saving policies can differentiate between "urgent" traffic and "background" traffic.

Initially, we planned to use a suite called DCPerf, a set of benchmarks from Meta that closely mimics datacenter workloads. While DCPerf is very realistic, it is designed to run on a "full-system" simulator, which simulated benchmarks too slow for our schedule. Instead, we switched to the SNIPER multi-core simulator. SNIPER allowed us to skip the slow parts of full-system simulation and generate network traces much faster.

\subsection{Trace Generation}
In order to simulate a NoC, we need to first generate traces from the benchmarks. We initially explored the gem5 simulator to perform full-system cycle-accurate simulations; however, due to significant time constraints and the immense computational overhead required for many-core configurations, we transitioned to the SNIPER multi-core simulator for the trace generation phase.

SNIPER is a a parallel multi-core simulator with the detail of cycle-level simulation that uses interval simulation. It groups instructions into intervals between major events like branch mispredictions or cache misses, which greatly saves simulation time. In comparison tests, Sniper has been shown to be the fastest and most accurate among contemporary x86 simulators like gem5 and PTLsim, though it is slightly less flexible for modeling entirely new hardware features. ~\cite{carlson_sniper_sc11}
This limitation is not a concern for our study. Since our primary goal is to generate benchmark network traces, SNIPER’s high speed and validated accuracy on x86 architectures make it the most effective tool for our infrastructure.

\subsection{Network Simulator}
The second half of our infrastructure uses BookSim 2.0, a cycle-accurate simulator specifically designed for the Network-on-Chip (NoC). Because we use pre-generated traces from SNIPER, we can run the same traffic patterns through BookSim over and over again while changing only the policies that modify power and frequency settings. This allows us to see precisely how a site-level power cap affects the latency of important data packets.

In addition to standard BookSim metrics, we model DVFS actuation either \emph{globally} (single DVFS
domain) or \emph{spatially} (multiple domains, up to per-router granularity). Each router maintains a
continuous \texttt{freq\_scale} that controls how many internal pipeline steps it can advance per BookSim
cycle. Lower \texttt{freq\_scale} therefore reduces the effective service rate of routing, VC allocation,
switch allocation, and crossbar traversal, increasing queueing delay and tail latency under load.

Policies execute at fixed DVFS epochs. During an epoch, the simulator collects telemetry including total
NoC power, per-router queue occupancies, injection/stall rates, and per-class latency percentiles. At the
epoch boundary, a policy computes the next frequency scale(s), clamped by \texttt{dvfs\_min\_scale} and
\texttt{dvfs\_max\_scale}, and applied to the configured \texttt{router\_domains}. This mirrors a
hardware/software DVFS governor where sensors are sampled periodically and new operating points are
programmed with bounded actuation granularity.

\subsection{Class-aware scheduling and the DVFS control loop}
Our simulator makes policies class-aware end-to-end. Each injected packet carries a \emph{class ID}
(synthetic traffic, or trace-defined via \texttt{node\_types} in netrace). A pluggable \texttt{ClassAssigner}
interface can override or infer classes based on source/destination or runtime signals; in our current
experiments we use static, trace-provided class labels. Per class, \texttt{ClassConfig} specifies a base
priority and an optional latency SLO in cycles. When an SLO is present, we track that class's tail latency
(\texttt{P99}) and treat it as the \emph{control class}.

Class awareness also appears in router scheduling. A \texttt{StaticPriorityPolicy} assigns each flit the
class's base priority at enqueue time, and a \texttt{DeadlineBoostPolicy} can temporarily increase a
class's priority when its measured \texttt{P99} exceeds its SLO. This creates a fast path for urgent traffic
at arbitration time, while DVFS adjusts the \emph{service rate} of the fabric.

Finally, DVFS actuation is applied through a \texttt{NetworkControl} adapter. Policies output either a
single domain scale (\texttt{SetDomainSpeed}) or per-router scales (\texttt{SetRouterSpeed}). Each router
tracks a continuous \texttt{freq\_scale}; lowering this value reduces the router pipeline progress executed
per simulator cycle, increasing effective per-hop service time and queueing. Domain and per-router bounds
are enforced via \texttt{dvfs\_min\_scale}, \texttt{dvfs\_max\_scale}, and a configurable \texttt{router\_domains}
partitioning. At each DVFS epoch, the simulator summarizes telemetry (power, occupancy, injection/stall
rates, and per-class latency percentiles) and the controller computes new scale(s) for the next epoch.

\section{Data}
\subsection{Trace Format}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=1.1]{traces.png}
    \caption{Example Trace Snippet}
    \label{fig:your_label}
\end{figure}

This is an example SNIPER-generated network trace that includes both batch-class and control-class workloads in a line network. This interleaving provides a pseudo-realistic representation of the system operating with both workload types simultaneously. The formatting of these traces is shown below.
\newline


\resizebox{200pt}{!}{
\begin{tabular}{|c|c|}
\hline
index 0 & cycle \# \\ \hline
index 1 & sender ID \\ \hline
index 2 & receiver ID \\ \hline
index 3 & packet size \\ \hline
index 4 & latency \\ \hline
index 5 & control (0) / batch (1) \\ \hline
\end{tabular}
}

\subsection{Metrics}
We collect both power and QoS-oriented performance telemetry at DVFS epoch boundaries:
(i) total NoC power under the current frequency configuration,
(ii) per-router input-buffer occupancy (and derived congestion signals),
(iii) per-router injection and stall rates,
(iv) per-class delivered-packet latency percentiles (P50/P95/P99),
and (v) per-class injection and throughput (delivered packets/flits per epoch).
These signals are sufficient to implement purely power-driven baselines (uniform throttling) as well as
class-aware controllers that explicitly protect control-class \texttt{P99} under a hard cap.

\section{DVFS Control Policies}
All DVFS controllers run in discrete \emph{DVFS epochs}. During an epoch, the NoC executes at the
currently programmed frequency scale(s). At the epoch boundary, the simulator summarizes telemetry
and the controller computes new scale(s) for the next epoch, subject to a hard NoC power budget.
We study two baseline policies and three increasingly class-aware controllers.

\subsection{Baselines}
\textbf{Static DVFS (no control).} This baseline fixes the NoC frequency scale (typically 1.0) and never
adapts. It serves as a reference point for latency and power when the control loop is disabled.

\textbf{Uniform throttling (power-cap baseline).} Uniform throttling enforces the NoC power cap by
applying a \emph{single global} frequency scale to all routers. If total NoC power exceeds the cap,
the policy reduces this global scale (e.g., linearly) until the estimated power meets the budget; if the
cap is not exceeded, it runs at the maximum scale. Uniform throttling is intentionally \emph{not}
class-aware: it reacts only to total power and cannot protect control-class tail latency.

\subsection{HWReactive: threshold governor with SLO override}
HWReactive models a lightweight hardware DVFS governor. Each epoch it measures a configurable congestion
signal $X$ (e.g., average queue occupancy, injection rate, stall rate, or observed latency) and compares it
to low/high thresholds $(T_{\text{low}}, T_{\text{high}})$ with hysteresis. In single-domain mode it toggles
between $(f_{\text{low}}, f_{\text{high}})$:
\begin{equation}
f[n\!+\!1] =
\begin{cases}
f_{\max} & \text{if } L^{\text{ctrl}}_{99}[n] > \text{SLO} \\
f_{\max} & \text{if } L^{\text{ctrl}}_{99}[n] \approx \text{SLO} \\
f_{\text{high}} & \text{if } X[n] > T_{\text{high}} \\
f_{\text{low}} & \text{if } X[n] < T_{\text{low}} \\
f[n] & \text{otherwise (hold)}
\end{cases}
\end{equation}
The key class-aware feature is the \emph{SLO override}: if the control class P99 latency exceeds (or is
close to) its target, the controller forces a high-frequency state to preserve slack.

In multi-domain (per-router) mode, HWReactive computes a per-router metric $M_i$ and normalizes it to a
weight $w_i$ (one simple choice is $w_i = \frac{M_i}{\sum_j M_j}$). It then interpolates each router's scale:
\begin{equation}
f_i = f_{\text{low}} + w_i \cdot (f_{\text{high}} - f_{\text{low}}), \qquad
f_i \in [f_{\text{low}}, f_{\text{high}}].
\end{equation}
Busy routers receive higher frequency, while idle routers are throttled more aggressively, concentrating
power where it most reduces queueing delay. The SLO override can still bias the overall allocation upward
when control-class tail latency is at risk.

\subsection{QueuePID: occupancy-regulating feedback loop with budget normalization}
QueuePID treats DVFS as service-rate control and regulates router input-buffer occupancy around a target.
Let $O_i[n]$ be the measured occupancy of router/domain $i$ at epoch $n$ and $T_i[n]$ be its target. The
error is $e_i[n] = O_i[n] - T_i[n]$. A per-router PID loop computes a frequency update:
\begin{align}
P_i[n] &= K_P e_i[n] \\
I_i[n] &= I_i[n-1] + K_I e_i[n] \\
D_i[n] &= K_D (e_i[n] - e_i[n-1]) \\
\Delta f_i[n] &= P_i[n] + I_i[n] + D_i[n] \\
f_i[n\!+\!1] &= \text{clamp}(f_i[n] + \Delta f_i[n], f_{\min}, f_{\max}).
\end{align}
To bias power toward hot spots, QueuePID uses \emph{adaptive occupancy targets}: routers with higher
relative load are assigned lower $T_i$ (forcing speed-up), while lightly loaded routers are assigned higher
$T_i$ (allowing slowdown). After computing candidate $f_i$, the controller enforces the global power cap by
a normalization step that preserves relative differences:
\begin{equation}
\text{if } P_{\text{NoC}} > P_{\text{cap}},\ \alpha = \frac{P_{\text{cap}}}{P_{\text{NoC}}},\ \ \forall i:\ f_i \leftarrow \alpha f_i.
\end{equation}
Class awareness enters through the same control-class P99 feedback used by HWReactive: when
$L^{\text{ctrl}}_{99}$ violates (or approaches) its SLO, QueuePID biases the update upward and can force
$f_{\max}$ in the single-domain case. Conceptually, occupancy regulation becomes subordinate to SLO
protection whenever they conflict.

\subsection{PerfTarget: direct P99 targeting}
PerfTarget closes the loop around the metric of interest: control-class tail latency. Let
$L^{\text{ctrl}}_{99}[n]$ be the measured control P99 at epoch $n$ and $L_{\text{target}}$ be the setpoint.
Define latency error $e_L[n] = L^{\text{ctrl}}_{99}[n] - L_{\text{target}}$. A simple proportional form is:
\begin{equation}
f[n\!+\!1] = \text{clamp}\Big(f[n] + K_L \cdot \frac{e_L[n]}{L_{\text{target}}},\ f_{\min}, f_{\max}\Big),
\end{equation}
implemented with step sizes and hysteresis in practice. If the control class has no completed packets in an
epoch (no P99 sample), the controller holds its previous decision rather than making a blind adjustment.
When the control class has ample slack and power headroom, PerfTarget gently reduces frequency to reclaim
power.

In per-router mode, PerfTarget uses occupancy as a spatial hint to \emph{localize} DVFS action under a tight
cap: when P99 is high, it boosts high-occupancy routers (likely on the critical path) while throttling cold
routers to remain within the budget. This approximates a constrained optimization of ``meet $L_{\text{target}}$
subject to $P_{\text{cap}}$'' without an expensive solver.

\subsection{Why multi-domain, class-aware DVFS helps}
Across all three controllers, multi-domain actuation is the key feature that uniform throttling is missing: it
allows the policy to \emph{reallocate} limited power toward routers that most influence queueing delay for
latency-critical traffic, while extracting power from regions that primarily affect batch throughput. Combined
with class-aware telemetry (control P99) and, optionally, priority boosting at arbitration, these policies aim
to preserve control-class tail latency under site-level caps where uniform DVFS would degrade all traffic
equally.

\section*{Acknowledgment}
We would like to express our sincere gratitude to Professor Sagar Karandikar for his guidance and support throughout this project. His expertise in the course CS 294-252: Architectures and Systems for Warehouse-Scale Computers was instrumental in shaping the direction of this research. We also want to thank my classmates for their valuable suggestions and feedback during our in-class discussions, which helped refine our approach to priority-aware power management. Finally, we are grateful for the resources provided by the University of California, Berkeley, which made this work possible.

\section*{Evaluation}

\section*{Conclusion}

\section*{Future Work}


\begin{thebibliography}{00}

\bibitem{li_capmaestro_hpca19}
Y.~Li, C.~R.~Lefurgy, K.~Rajamani, M.~S.~Allen-Ware, G.~J.~Silva, D.~D.~Heimsoth, S.~Ghose, and O.~Mutlu,
``A Scalable Priority-Aware Approach to Managing Data Center Server Power,''
in \emph{Proc. IEEE Int'l Symp. High-Performance Computer Architecture (HPCA)}, 2019.

\bibitem{li_thunderbolt_osdi20}
S.~Li, X.~Wang, X.~Zhang, V.~Kontorinis, S.~Kodakara, D.~Lo, and P.~Ranganathan,
``Thunderbolt: Throughput-Optimized, Quality-of-Service-Aware Power Capping at Scale,''
in \emph{Proc. USENIX Symp. Operating Systems Design and Implementation (OSDI)}, 2020.

\bibitem{bhattacharya_speed_stability_igcc12}
A.~Bhattacharya, J.~M.~Culler, R.~Kansal, S.~Govindan, and S.~Sankar,
``The Need for Speed and Stability in Data Center Power Capping,''
in \emph{Proc. Int'l Green Computing Conf. (IGCC)}, 2012.

\bibitem{kasture_rubik_micro15}
H.~Kasture, D.~Bartolini, N.~Beckmann, and D.~Sanchez,
``Rubik: Fast Analytical Power Management for Latency-Critical Systems,''
in \emph{Proc. IEEE/ACM Int'l Symp. Microarchitecture (MICRO)}, 2015.

\bibitem{adhinarayanan_interconnect_power_iiswc16}
V.~Adhinarayanan, I.~Paul, J.~L.~Greathouse, W.~Huang, A.~Pattnaik, and W.-c.~Feng,
``Measuring and Modeling On-Chip Interconnect Power on Real Hardware,''
in \emph{Proc. IEEE Int'l Symp. Workload Characterization (IISWC)}, 2016.

\bibitem{carlson_sniper_sc11}
T.~E.~Carlson, W.~Heirman, and L.~Eeckhout,
``Sniper: Exploring the Level of Abstraction for Scalable and Accurate Parallel Multi-core Simulations,''
in \emph{Proc. Int'l Conf. for High Performance Computing, Networking, Storage and Analysis (SC)}, 2011.

\bibitem{jiang_booksim_ispass13}
N.~Jiang, D.~U.~Becker, G.~Michelogiannakis, J.~Balfour, B.~Towles, D.~E.~Shaw, J.~Kim, and W.~J.~Dally,
``A Detailed and Flexible Cycle-Accurate Network-on-Chip Simulator,''
in \emph{Proc. IEEE Int'l Symp. Performance Analysis of Systems and Software (ISPASS)}, 2013.

\bibitem{simunic_power_noc_tvlsi04}
T.~Simunic, S.~P.~Boyd, and P.~Glynn,
``Managing Power Consumption in Networks on Chips,''
\emph{IEEE Trans. Very Large Scale Integration (VLSI) Systems}, vol.~12, no.~1, pp.~96--107, 2004.

\bibitem{hesse_coherence_dvfs}
R.~Hesse and N.~E.~Jerger,
``Improving DVFS in NoCs with Coherence Prediction,''
in \emph{Proc. IEEE Int'l Symp. Networks-on-Chip (NOCS)}, 2015.

\bibitem{sharifi_pepon_pact12}
A.~Sharifi, A.~K.~Mishra, S.~Srikantaiah, M.~T.~Kandemir, and C.~R.~Das,
``PEPON: Performance-Aware Hierarchical Power Budgeting for NoC based Multicores,''
in \emph{Proc. Int'l Conf. Parallel Architectures and Compilation Techniques (PACT)}, 2012.

\bibitem{bogdan_optimal_control_todaes12}
P.~Bogdan and R.~Marculescu,
``Towards a Science of Cyber-Physical Systems Design: A Time and Energy Perspective,''
\emph{ACM Trans. Design Automation of Electronic Systems}, vol.~17, no.~3, 2012.

\bibitem{amd_versal_qos}
AMD,
``NoC and QoS Requirements (UG994),'' documentation, 2025.

\bibitem{mishra_freqtuning_micro09}
A.~K.~Mishra, R.~Das, S.~Eachempati, R.~R.~Iyer, V.~Narayanan, and C.~R.~Das,
``A Case for Dynamic Frequency Tuning in On-Chip Networks,''
in \emph{Proc. IEEE/ACM Int'l Symp. Microarchitecture (MICRO)}, 2009.

\bibitem{casu_date2015}
M.~R.~Casu and P.~Giaccone,
``Rate-based vs Delay-based Control for DVFS in NoC,''
in \emph{Proc. Design, Automation \& Test in Europe (DATE)}, 2015.

\bibitem{casu_jpdc2017}
M.~R.~Casu and P.~Giaccone,
``Power-performance assessment of different DVFS control policies in NoCs,''
\emph{Journal of Parallel and Distributed Computing}, 2017.

\bibitem{juang_islped2005}
P.~Juang, K.~Skadron, M.~Martonosi, and D.~Clark,
``Coordinated, Distributed, Formal Energy Management of Chip Multiprocessors,''
in \emph{Proc. Int'l Symp. Low Power Electronics and Design (ISLPED)}, 2005.

\end{thebibliography}
\vspace{12pt}
\end{document}
