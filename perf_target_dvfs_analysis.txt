Mostly **yes — but only if your perf target is already being met (so the policy never “does” anything), and also because this policy does *not* actually enforce the power cap.** Your table has a couple of dead giveaways:

### What your results are saying
- **Performance is identical across all power caps and injection rates**:
  - `control_p50/p95/p99` are always `11/24/34`
  - `throughput_flits_class0` is always `0.388053`
  - `stall_xbar_rate_class0` is constant within each `dvfs_epoch` group
- **Power is also identical across power caps** (within a given `dvfs_epoch`):
  - For `dvfs_epoch=5000`: `total_power_peak` is always `1.4868`
  - For `dvfs_epoch=10000`: `total_power_peak` is always `1.05305`

So **changing `power_cap` is not changing behavior** — it’s only changing the *reported* “headroom”.

### The smoking gun: `headroom_min` is just `power_cap - total_power_peak`
Pick any row (e.g., cap 1.0 with peak 1.4868):  
`1.0 - 1.4868 = -0.4868` → matches your `headroom_min ≈ -0.486797`.

That means: **the cap isn’t constraining the sim/policy; you’re just computing headroom from it afterward (or telemetry is), while power stays the same.**

### Does that match your policy code?
Yes, it can.

#### 1) In `latency` mode you *never* slow down
You changed latency control to:
```cpp
double err = std::max(0.0, meas - _target);
double delta = _kp * (err / _target);
return clamp(old_scale + delta);
```
So if `meas <= target`, then `err=0` → `delta=0` → **scale never changes**.

If your target is something like **70 cycles** and your measured p99 is **34**, you will *always* get `err=0`. That perfectly explains “flat” results.

Also: since you don’t produce a negative delta, **you also never reduce frequency to save power**, even when you have tons of slack.

#### 2) Your policy does not enforce a cap (it only gates *increases*)
This is subtle but important:

- `headroom_ok()` can block *increases* when `pwr.headroom` is too small.
- But if you’re already over cap (negative headroom), **nothing forces a decrease** unless `decide()` produces a smaller `new_scale`.

In `latency` mode, `decide()` never decreases, so:
- If baseline power at scale=1.0 violates cap → you will **continue violating cap forever**.
- That matches your negative `headroom_min` for low caps.

### Two things that look “off” (worth checking)
- **`exit_code=255` everywhere**: if 255 means “aborted”, these metrics might be partial / default / from early windows.
- **`dvfs_epoch` changes power peak a lot (1.49 → 1.05) without changing latency/throughput at all**. If DVFS scale truly changed that much, you’d usually expect *some* perf movement—unless:
  - your perf metrics are measured in a way that’s not sensitive to your frequency scaling, or
  - the runs are ending early / not reaching steady state, or
  - you have a bug where “power” is affected by `dvfs_epoch` (sampling/integration), but “latency” isn’t.

### Quick sanity checks to confirm what’s happening
1) **Log the chosen scale every update** (domain speed or router speed), plus whether headroom gating blocked it:
- `meas, target, err, delta, old_scale, new_scale, pwr.headroom, headroom_ok`
2) Verify your config knobs:
- Are `min_scale` and `max_scale` accidentally both `1.0`?
- Is `_metric` actually `"latency"` (lowercased), or something else that’s forcing the throughput path?
- Is `_target` set so high that you never violate it?

### If you want this policy to respect a power cap
You need an explicit “cap-backoff” path, e.g.:
- If `pwr.headroom < 0` (over cap), **force a decrease** regardless of perf target.
- Then separately do perf restoration when you have headroom.

Right now, with latency-as-SLO (`max(0, meas-target)`), you’ve effectively implemented **“only speed up on SLO violation”**, not **“hit a perf target under a cap.”**

If you paste your perf_target config parameters (metric/target/kp/min/max/headroom_margin/per_router), I can tell you exactly which branch you’re in and why you’re getting the totally flat table.
